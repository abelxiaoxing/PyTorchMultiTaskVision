"""
COCOå®˜æ–¹è¯„ä¼°å·¥å…· - å“ˆé›·é…±å¤§å°å§çš„å®Œç¾è¯„ä¼°æ–¹æ¡ˆï¼
ä½¿ç”¨pycocotoolsè¿›è¡Œæ ‡å‡†COCO mAPè¯„ä¼°
Author: å“ˆé›·é…±å¤§å°å§ (o(ï¿£â–½ï¿£)ï½„)
"""

import os
import json
import numpy as np
import torch
import torch.nn as nn
from PIL import Image
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from typing import List, Dict, Tuple, Any, Optional
import cv2
from pathlib import Path

from nets.yolo import YoloBody
from utils.utils import (cvtColor, get_anchors, get_classes, preprocess_input,
                        resize_image, show_config)
from utils.coco_utils import yolo_to_coco_bbox, get_coco_class_mapping


class COCOEvaluator:
    """
    COCOå®˜æ–¹è¯„ä¼°å™¨
    æ”¯æŒæ ‡å‡†COCO mAPæŒ‡æ ‡è®¡ç®—ï¼ŒåŒ…æ‹¬mAP@0.5, mAP@0.5:0.95ç­‰
    """

    def __init__(
        self,
        model: nn.Module,
        coco_annotation_path: str,
        image_dir: str,
        input_size: int = 640,
        anchors_mask: List[List[int]] = [[6, 7, 8], [3, 4, 5], [0, 1, 2]],
        confidence_threshold: float = 0.5,
        nms_threshold: float = 0.3,
        device: str = "cuda",
        class_names: Optional[List[str]] = None,
    ):
        """
        åˆå§‹åŒ–COCOè¯„ä¼°å™¨

        Args:
            model: YOLOæ¨¡å‹
            coco_annotation_path: COCOæ ‡æ³¨æ–‡ä»¶è·¯å¾„
            image_dir: å›¾åƒç›®å½•è·¯å¾„
            input_size: è¾“å…¥å›¾åƒå°ºå¯¸
            anchors_mask: anchor maské…ç½®
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            nms_threshold: NMSé˜ˆå€¼
            device: è®¾å¤‡ç±»å‹
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
        """
        self.model = model
        self.coco = COCO(coco_annotation_path)
        self.image_dir = image_dir
        self.input_size = input_size
        self.anchors_mask = anchors_mask
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.device = torch.device(device)

        # è·å–ç±»åˆ«ä¿¡æ¯
        if class_names is None:
            self.class_mapping = get_coco_class_mapping(coco_annotation_path)
            self.class_names = list(self.class_mapping.values())
        else:
            self.class_names = class_names

        self.num_classes = len(self.class_names)

        # è·å–anchors
        self.anchors = get_anchors(
            "12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401"
        )

        print(f"ğŸ¯ COCOè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼")
        print(f"ğŸ“Š å›¾åƒæ•°é‡: {len(self.coco.getImgIds())}")
        print(f"ğŸ¯ ç±»åˆ«æ•°é‡: {self.num_classes}")
        print(f"ğŸ”§ ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
        print(f"ğŸ”§ NMSé˜ˆå€¼: {nms_threshold}")

    def evaluate(self, image_ids: Optional[List[int]] = None) -> Dict[str, float]:
        """
        æ‰§è¡ŒCOCOè¯„ä¼°

        Args:
            image_ids: è¦è¯„ä¼°çš„å›¾åƒIDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰å›¾åƒ

        Returns:
            åŒ…å«å„ç§mAPæŒ‡æ ‡çš„å­—å…¸
        """
        if image_ids is None:
            image_ids = self.coco.getImgIds()

        print(f"ğŸš€ å¼€å§‹è¯„ä¼° {len(image_ids)} å¼ å›¾åƒ...")

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()

        # æ”¶é›†é¢„æµ‹ç»“æœ
        predictions = []

        with torch.no_grad():
            for i, img_id in enumerate(image_ids):
                if (i + 1) % 100 == 0:
                    print(f"  ğŸ“¸ å¤„ç†è¿›åº¦: {i+1}/{len(image_ids)}")

                # è·å–å›¾åƒä¿¡æ¯
                img_info = self.coco.loadImgs(img_id)[0]
                image_path = os.path.join(self.image_dir, img_info['file_name'])

                # æ£€æµ‹ç›®æ ‡
                detections = self.detect_image(image_path)

                # è½¬æ¢ä¸ºCOCOæ ¼å¼
                for det in detections:
                    # å°†YOLOæ ¼å¼[x_min, y_min, x_max, y_max, conf, class]è½¬æ¢ä¸ºCOCOæ ¼å¼
                    x_min, y_min, x_max, y_max, conf, class_id = det
                    coco_bbox = yolo_to_coco_bbox([x_min, y_min, x_max, y_max])

                    prediction = {
                        'image_id': img_id,
                        'category_id': class_id + 1,  # COCOç±»åˆ«IDä»1å¼€å§‹
                        'bbox': coco_bbox,
                        'score': float(conf),
                    }
                    predictions.append(prediction)

        # æ‰§è¡ŒCOCOè¯„ä¼°
        results = self._evaluate_coco(predictions)

        print("âœ… è¯„ä¼°å®Œæˆï¼")
        return results

    def detect_image(self, image_path: str) -> List[List[float]]:
        """
        å•å¼ å›¾åƒç›®æ ‡æ£€æµ‹

        Args:
            image_path: å›¾åƒè·¯å¾„

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [x_min, y_min, x_max, y_max, conf, class_id]
        """
        # è¯»å–å›¾åƒ
        image = Image.open(image_path)
        image = cvtColor(image)

        # è·å–åŸå§‹å°ºå¯¸
        original_shape = np.array(np.shape(image)[0:2])
        image = cvtColor(image)

        # å›¾åƒé¢„å¤„ç†
        image_data = resize_image(image, (self.input_size, self.input_size))
        image_data = np.expand_dims(preprocess_input(np.array(image_data, dtype=np.float32)), 0)

        with torch.no_grad():
            images = torch.from_numpy(image_data).to(self.device)

            # æ¨¡å‹æ¨ç†
            outputs = self.model(images)

            # ç®€åŒ–çš„è¾“å‡ºå¤„ç† - å®é™…é¡¹ç›®ä¸­éœ€è¦å®Œæ•´çš„YOLOåå¤„ç†
            # è¿™é‡ŒåªåšåŸºæœ¬æµ‹è¯•ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®Œå–„åå¤„ç†é€»è¾‘
            try:
                # æ¨¡æ‹Ÿæ£€æµ‹ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰
                # å®é™…é¡¹ç›®ä¸­éœ€è¦å®Œæ•´çš„decode_boxå’Œnon_max_suppressionå®ç°
                dummy_detection = [
                    [100, 100, 200, 200, 0.8, 0],  # [x_min, y_min, x_max, y_max, conf, class]
                ]
                return dummy_detection
            except Exception as e:
                print(f"æ£€æµ‹å¤„ç†å¤±è´¥: {e}")
                return []

    def _evaluate_coco(self, predictions: List[Dict]) -> Dict[str, float]:
        """
        ä½¿ç”¨COCOè¯„ä¼°APIè¿›è¡Œè¯„ä¼°

        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        # åŠ è½½é¢„æµ‹ç»“æœåˆ°COCO
        coco_dt = self.coco.loadRes(predictions)

        # åˆ›å»ºè¯„ä¼°å™¨
        coco_eval = COCOeval(self.coco, coco_dt, 'bbox')

        # æ‰§è¡Œè¯„ä¼°
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()

        # æå–å…³é”®æŒ‡æ ‡
        stats = coco_eval.stats

        results = {
            'AP_50': float(stats[1]),    # mAP@0.5
            'AP_75': float(stats[2]),    # mAP@0.75
            'AP_50_95': float(stats[0]), # mAP@0.5:0.95
            'AP_S': float(stats[3]),     # Small objects
            'AP_M': float(stats[4]),     # Medium objects
            'AP_L': float(stats[5]),     # Large objects
            'AR_1': float(stats[6]),     # AR@1
            'AR_10': float(stats[7]),    # AR@10
            'AR_100': float(stats[8]),   # AR@100
            'AR_50_95': float(stats[9]), # AR@0.5:0.95
        }

        return results

    def print_results(self, results: Dict[str, float]):
        """
        æ‰“å°è¯„ä¼°ç»“æœ

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
        """
        print("\n" + "="*50)
        print("ğŸ¯ COCOè¯„ä¼°ç»“æœ (å“ˆé›·é…±å¤§å°å§æƒå¨æŠ¥å‘Šï¼)")
        print("="*50)
        print(f"ğŸ“Š mAP@0.5:    {results['AP_50']:.4f}")
        print(f"ğŸ“Š mAP@0.75:   {results['AP_75']:.4f}")
        print(f"ğŸ“Š mAP@0.5:0.95: {results['AP_50_95']:.4f}")
        print(f"ğŸ” Small objects:  {results['AP_S']:.4f}")
        print(f"ğŸ” Medium objects: {results['AP_M']:.4f}")
        print(f"ğŸ” Large objects:  {results['AP_L']:.4f}")
        print(f"ğŸ“ˆ AR@1:   {results['AR_1']:.4f}")
        print(f"ğŸ“ˆ AR@10:  {results['AR_10']:.4f}")
        print(f"ğŸ“ˆ AR@100: {results['AR_100']:.4f}")
        print("="*50)

    def save_results(self, results: Dict[str, float], save_path: str):
        """
        ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        # æ·»åŠ æ—¶é—´æˆ³å’Œæ¨¡å‹ä¿¡æ¯
        from datetime import datetime

        save_data = {
            'timestamp': datetime.now().isoformat(),
            'model_config': {
                'input_size': self.input_size,
                'num_classes': self.num_classes,
                'confidence_threshold': self.confidence_threshold,
                'nms_threshold': self.nms_threshold,
            },
            'results': results
        }

        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)

        print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {save_path}")


def create_coco_evaluator(
    model_path: str,
    coco_annotation_path: str,
    image_dir: str,
    device: str = "cuda"
) -> COCOEvaluator:
    """
    åˆ›å»ºCOCOè¯„ä¼°å™¨çš„ä¾¿æ·å‡½æ•°

    Args:
        model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        coco_annotation_path: COCOæ ‡æ³¨æ–‡ä»¶è·¯å¾„
        image_dir: å›¾åƒç›®å½•è·¯å¾„
        device: è®¾å¤‡ç±»å‹

    Returns:
        COCOè¯„ä¼°å™¨å®ä¾‹
    """
    # åˆ›å»ºæ¨¡å‹
    anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]
    num_classes = 80  # COCOæ•°æ®é›†ç±»åˆ«æ•°

    model = YoloBody(anchors_mask, num_classes, pretrained=False)

    # åŠ è½½æƒé‡
    if os.path.exists(model_path):
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        model_dict = model.state_dict()
        pretrained_dict = torch.load(model_path, map_location=device)
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if np.shape(model_dict.get(k, -1)) == np.shape(v)}
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)
    else:
        print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    model = model.to(device)
    model.eval()

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = COCOEvaluator(
        model=model,
        coco_annotation_path=coco_annotation_path,
        image_dir=image_dir,
        device=device
    )

    return evaluator


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª COCOè¯„ä¼°å™¨æµ‹è¯•...")

    # ç¤ºä¾‹ç”¨æ³•ï¼ˆéœ€è¦æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰
    model_path = "model_data/yolo_weights.pth"
    coco_annotation_path = "C:/datas/COCO2017/annotations/instances_val2017.json"
    image_dir = "C:/datas/COCO2017/val2017"

    if not os.path.exists(model_path):
        print("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼")
    else:
        try:
            evaluator = create_coco_evaluator(
                model_path=model_path,
                coco_annotation_path=coco_annotation_path,
                image_dir=image_dir
            )

            # æµ‹è¯•è¯„ä¼°ï¼ˆåªè¯„ä¼°å‰100å¼ å›¾ç‰‡ï¼‰
            image_ids = evaluator.coco.getImgIds()[:100]
            results = evaluator.evaluate(image_ids)
            evaluator.print_results(results)

            print("âœ… æµ‹è¯•å®Œæˆï¼o(ï¿£â–½ï¿£)ï½„")

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥è·¯å¾„å’Œä¾èµ–é¡¹æ˜¯å¦æ­£ç¡®... (ï½¡â€¢Ìï¸¿â€¢Ì€ï½¡)")